trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  min_epochs: 1
  max_epochs: 3
  progress_bar_refresh_rate: null
  weights_summary: null
  profiler: null
  gradient_clip_val: 0
  accumulate_grad_batches: 1
  val_check_interval: 0.05
  check_val_every_n_epoch: 1
  stochastic_weight_avg: false
  num_sanity_val_steps: 2
  fast_dev_run: false
  overfit_batches: 0
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  track_grad_norm: -1
  terminate_on_nan: false
  amp_backend: native
  amp_level: '02'
  precision: 32
  accelerator: null
  num_nodes: 1
  tpu_cores: null
  deterministic: false
  resume_from_checkpoint: null
model:
  _target_: src.models.mnist_mlp_model.MNISTLitModel
  input_size: 784
  lin1_size: 512
  lin2_size: 512
  lin3_size: 512
  output_size: 10
  lr: 0.001
  weight_decay: 0.0005
  gpu: true
datamodule:
  _target_: src.datamodules.mnist_datamodule.MNISTDataModule
  data_dir: ${data_dir}
  batch_size: 64
  train_val_test_split:
  - 60000
  - 10000
  - 0
  num_workers: 0
  pin_memory: false
  deskew: false
  fashionmnist: true
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/acc
    save_top_k: 1
    save_last: true
    mode: max
    verbose: false
    dirpath: checkpoints/
    filename: '{epoch:02d}'
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/acc
    patience: 100
    mode: max
    min_delta: 0
logger:
  csv:
    _target_: pytorch_lightning.loggers.csv_logs.CSVLogger
    save_dir: .
    name: '2'
work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data/
debug: false
print_config: true
disable_warnings: true
