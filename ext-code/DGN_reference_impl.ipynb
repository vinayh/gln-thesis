{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s54x-Gq4AiYb"
      },
      "source": [
        "## Simple Dendritic Gated Networks in numpy\n",
        "\n",
        "This colab implements a Dendritic Gated Network (DGN) solving a regression (using quadratic loss) or a binary classification problem (using Bernoulli log loss).\n",
        "\n",
        "See our paper titled [\"A rapid and efficient learning rule for biological neural circuits\"](https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1) for details of the DGN model.\n",
        "\n",
        "\n",
        "Some implementation details:\n",
        "- We utilize `sklearn.datasets.load_breast_cancer` for binary classification and `sklearn.datasets.load_diabetes` for regression.\n",
        "- This code is meant for educational purposes only. It is not optimized for high-performance, both in terms of computational efficiency and quality of fit.\n",
        "- Network is trained on 80% of the dataset and tested on the rest. For classification, we report log loss (negative log likelihood) and accuracy (percentage of correctly identified labels). For regression, we report MSE expressed in units of target variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jhiajfn0EAxE"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 DeepMind Technologies Limited. All rights reserved.\n",
        "#\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nm-F_uZA0_T2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from typing import List, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOoiBATk1AgQ"
      },
      "source": [
        "## Choose classification or regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FCjzwzwh0ycl"
      },
      "outputs": [],
      "source": [
        "do_classification = True  # if False, does regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA5VmSeV-GTc"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qnzNZrzNk3Pl"
      },
      "outputs": [],
      "source": [
        "if do_classification:\n",
        "  # features, targets = datasets.load_breast_cancer(return_X_y=True)\n",
        "  features, targets = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, cache=True)\n",
        "else:\n",
        "  features, targets = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "    features, targets, test_size=0.2, random_state=0)\n",
        "n_features = x_train.shape[-1]\n",
        "\n",
        "# Input features are centered and scaled to unit variance:\n",
        "feature_encoder = preprocessing.StandardScaler()\n",
        "x_train = feature_encoder.fit_transform(x_train)\n",
        "x_test = feature_encoder.transform(x_test)\n",
        "\n",
        "if not do_classification:\n",
        "  # Continuous targets are centered and scaled to unit variance:\n",
        "  target_encoder = preprocessing.StandardScaler()\n",
        "  y_train = np.squeeze(target_encoder.fit_transform(y_train[:, np.newaxis]))\n",
        "  y_test = np.squeeze(target_encoder.transform(y_test[:, np.newaxis]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTQxvDcok86S"
      },
      "source": [
        "## DGN inference/update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "F6Yt_tw0lmf1"
      },
      "outputs": [],
      "source": [
        "def step_square_loss(inputs: np.ndarray,\n",
        "                     weights: List[np.ndarray],\n",
        "                     hyperplanes: List[np.ndarray],\n",
        "                     hyperplane_bias_magnitude: Optional[float] = 1.,\n",
        "                     learning_rate: Optional[float] = 1e-5,\n",
        "                     target: Optional[float] = None,\n",
        "                     update: bool = False,\n",
        "                     ):\n",
        "  \"\"\"Implements a DGN inference/update using square loss.\"\"\"\n",
        "  r_in = inputs\n",
        "  side_info = np.hstack([hyperplane_bias_magnitude, inputs])\n",
        "\n",
        "  for w, h in zip(weights, hyperplanes):  # loop over layers\n",
        "    r_in = np.hstack([1., r_in])  # add biases\n",
        "    gate_values = np.heaviside(h.dot(side_info), 0).astype(bool)\n",
        "    effective_weights = gate_values.dot(w).sum(axis=1)\n",
        "    r_out = effective_weights.dot(r_in)\n",
        "\n",
        "    if update:\n",
        "      grad = (r_out[:, None] - target) * r_in[None]\n",
        "      w -= learning_rate * gate_values[:, :, None] * grad[:, None]\n",
        "\n",
        "    r_in = r_out\n",
        "  loss = (target - r_out)**2 / 2\n",
        "  return r_out, loss\n",
        "\n",
        "def sigmoid(x):  # numerically stable sigmoid\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def inverse_sigmoid(x):\n",
        "  return np.log(x/(1-x))\n",
        "\n",
        "def step_bernoulli(inputs: np.ndarray,\n",
        "                   weights: List[np.ndarray],\n",
        "                   hyperplanes: List[np.ndarray],\n",
        "                   hyperplane_bias_magnitude: Optional[float] = 1.,\n",
        "                   learning_rate: Optional[float] = 1e-5,\n",
        "                   epsilon: float = 0.01,\n",
        "                   target: Optional[float] = None,\n",
        "                   update: bool = False,\n",
        "                   ):\n",
        "  \"\"\"Implements a DGN inference/update using Bernoulli log loss.\"\"\"\n",
        "  r_in = np.clip(sigmoid(inputs), epsilon, 1-epsilon)\n",
        "  side_info = np.hstack([hyperplane_bias_magnitude, inputs])\n",
        "\n",
        "  for w, h in zip(weights, hyperplanes):  # loop over layers\n",
        "    r_in = np.hstack([sigmoid(1.), r_in])  # add biases\n",
        "    h_in = inverse_sigmoid(r_in)\n",
        "    gate_values = np.heaviside(h.dot(side_info), 0).astype(bool)\n",
        "    effective_weights = gate_values.dot(w).sum(axis=1)\n",
        "    h_out = effective_weights.dot(h_in)\n",
        "    r_out_unclipped = sigmoid(h_out)\n",
        "    r_out = np.clip(r_out_unclipped, epsilon, 1 - epsilon)\n",
        "    if update:\n",
        "      update_indicator = np.abs(target - r_out_unclipped) > epsilon\n",
        "      grad = (r_out[:, None] - target) * h_in[None]  * update_indicator[:, None]\n",
        "      w -= learning_rate * gate_values[:, :, None] * grad[:, None]\n",
        "    r_in = r_out\n",
        "  loss = - (target * np.log(r_out) + (1 - target) * np.log(1 - r_out))\n",
        "  return r_out, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0B7wSn3Azcfb"
      },
      "outputs": [],
      "source": [
        "def forward_pass(step_fn, x, y, weights, hyperplanes, learning_rate, update):\n",
        "  losses, outputs = np.zeros(len(y)), np.zeros(len(y))\n",
        "  for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
        "    outputs[i], losses[i] = step_fn(x_i, weights, hyperplanes, target=y_i,\n",
        "                                    learning_rate=learning_rate, update=update)\n",
        "  return np.mean(losses), outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41aHT8G0lsuu"
      },
      "source": [
        "## Define architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WSbPuwzFvV2N"
      },
      "outputs": [],
      "source": [
        "# number of neurons per layer, the last element must be 1\n",
        "n_neurons = np.array([100, 10, 1])\n",
        "n_branches = 20  # number of dendritic brancher per neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTk1YDXV-xoD"
      },
      "source": [
        "## Initialise weights and gating parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Uek-2I5IlyN3"
      },
      "outputs": [],
      "source": [
        "n_inputs = np.hstack([n_features + 1, n_neurons[:-1] + 1])  # 1 for the bias\n",
        "dgn_weights = [np.zeros((n_neuron, n_branches, n_input))\n",
        "               for n_neuron, n_input in zip(n_neurons, n_inputs)]\n",
        "\n",
        "# Fixing random seed for reproducibility:\n",
        "np.random.seed(12345)\n",
        "dgn_hyperplanes = [\n",
        "    np.random.normal(0, 1, size=(n_neuron, n_branches, n_features + 1))\n",
        "    for n_neuron in n_neurons]\n",
        "# By default, the weight parameters are drawn from a normalised Gaussian:\n",
        "dgn_hyperplanes = [\n",
        "    h_ / np.linalg.norm(h_[:, :, :-1], axis=(1, 2))[:, None, None]\n",
        "    for h_ in dgn_hyperplanes]\n",
        "\n",
        "# print(dgn_weights[0].shape)\n",
        "# print(n_features)\n",
        "# print(dgn_hyperplanes[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy1XUdaSm0ID"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wublBSqiucQ-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on classification problem for 3 epochs with learning rate 0.0001.\nThis may take a minute. Please be patient...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d83ed0ac3e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   train_loss, train_pred = forward_pass(\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       dgn_hyperplanes, eta, update=(epoch > 0))\n",
            "\u001b[0;32m<ipython-input-14-59f6d29687fa>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(step_fn, x, y, weights, hyperplanes, learning_rate, update)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     outputs[i], losses[i] = step_fn(x_i, weights, hyperplanes, target=y_i,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                     learning_rate=learning_rate, update=update)\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-da1604398ed3>\u001b[0m in \u001b[0;36mstep_bernoulli\u001b[0;34m(inputs, weights, hyperplanes, hyperplane_bias_magnitude, learning_rate, epsilon, target, update)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgate_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mr_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mr_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')"
          ]
        }
      ],
      "source": [
        "if do_classification:\n",
        "  eta = 1e-4\n",
        "  n_epochs = 3\n",
        "  step = step_bernoulli\n",
        "else:\n",
        "  eta = 1e-5\n",
        "  n_epochs = 10\n",
        "  step = step_square_loss\n",
        "\n",
        "if do_classification:\n",
        "  step = step_bernoulli\n",
        "else:\n",
        "  step = step_square_loss\n",
        "\n",
        "print('Training on {} problem for {} epochs with learning rate {}.'.format(\n",
        "    ['regression', 'classification'][do_classification], n_epochs, eta))\n",
        "print('This may take a minute. Please be patient...')\n",
        "\n",
        "for epoch in range(0, n_epochs + 1):\n",
        "  train_loss, train_pred = forward_pass(\n",
        "      step, x_train, y_train, dgn_weights,\n",
        "      dgn_hyperplanes, eta, update=(epoch > 0))\n",
        "\n",
        "  test_loss, test_pred = forward_pass(\n",
        "      step, x_test, y_test, dgn_weights,\n",
        "      dgn_hyperplanes, eta, update=False)\n",
        "  to_print = 'epoch: {}, test loss: {:.3f} (train: {:.3f})'.format(\n",
        "      epoch, test_loss, train_loss)\n",
        "\n",
        "  if do_classification:\n",
        "    accuracy_train = np.mean(np.round(train_pred) == y_train)\n",
        "    accuracy = np.mean(np.round(test_pred) == y_test)\n",
        "    to_print += ', test accuracy: {:.3f} (train: {:.3f})'.format(\n",
        "        accuracy, accuracy_train)\n",
        "  print(to_print)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "dendritic_gated_network.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1lzQUssVJpeziFs1fdBHueD7DqNp6lkVK",
          "timestamp": 1614705435731
        }
      ]
    },
    "kernelspec": {
      "name": "python392jvsc74a57bd0a17ed17fad220a131aa6f5725105be8c48ee3d3744905b4cc4c42118428a0cc1",
      "display_name": "Python 3.9.2 64-bit ('thesis': conda)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}